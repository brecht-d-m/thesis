# Spark Cluster / Job Server configuration
spark {
  # spark.master will be passed to each job's JobContext
  #master = "local[4]"
  master = "spark://10.20.32.1:7077"

  # Default # of CPUs for jobs to use for Spark standalone cluster
  job-number-cpus = 4

  jobserver {
    port = 8090
    jar-store-rootdir = /tmp/jobserver/jars

    context-per-jvm = true

    jobdao = spark.jobserver.io.JobFileDAO

    filedao {
      rootdir = /tmp/spark-job-server/filedao/data
    }
  }

  context-settings {
    num-cpu-cores = 20             # Number of cores to allocate.  Required.
    memory-per-node = 8G         # Executor memory per node, -Xmx style eg 512m, #1G, etc.
  }

  home = "/opt/spark"
}

spray.con.server {
    idle-timeout = infinite
    request-timeout = infinite
}

HDT Library, Java Implementation. http://www.rdfhdt.org

Overview
=================

HDT-lib is a Java Library that implements the W3C Submission (http://www.w3.org/Submission/2011/03/) of the RDF HDT (Header-Dictionary-Triples) binary format for publishing and exchanging RDF data at large scale. Its compact representation allows storing RDF in fewer space, providing at the same time direct access to the stored information. This is achieved by depicting the RDF graph in terms of three main components: Header, Dictionary and Triples. The Header includes extensible metadata required to describe the RDF data set and details of its internals. The Dictionary organizes the vocabulary of strings present in the RDF graph by assigning numerical IDs to each different string. The Triples component comprises the internal structure of the RDF graph in a compressed form.

It provides two components:
- Java Library: Provides an API to use HDT files programmatically. It allows creating HDT files from RDF and converting HDT files back to RDF. It also provides a Search interface to find triples that match a specific triple pattern.
- Command line tools. Allow to convert between RDF and HDT, and also perform searches against HDT files.

Including HDT Library to classpath
=========================================

Dependencies for searching:
The HDT library itself does not have any dependency to load and search HDT files, you just need to include the hdt-lib.jar. 

Dependencies for converting:
For parsing NTriples files you don't need any additional dependency, just the hdt-lib.jar.
For parsing RDF files in other formats (Turtle, N3, RDF-XML...) you will need the jars from Jena-core and Jena-arq.
The commandline tools use JCommander for parsing the arguments.



Command line tools
=====================================

The tool provides five main command line tools:

** The tool rdf2hdt converts an RDF file to HDT format. The format of the input file will be NTriples by default, although it can be set by using the "-f" flag.
** NOTE: rdf2hdt decompresses on the fly input files that end with ".gz".
** NOTE: Generating and loading big HDT Datasets can take a considerable amount of memory. Adjust the JVM heap memory using -Xmx flag according to your machine's memory in the launch scripts (.sh and .bat).
$ Usage: rdf2hdt [options] <Input RDF> <Output HDT>
  Options:
    -base      Base URI for the dataset
    -config    Conversion config file
    -options   HDT Conversion options
    -rdftype   Type of RDF Input (ntriples, n3, rdfxml)


** The tool hdt2rdf converts an HDT file back to RDF in NTriples format.
$ hdt2rdf <HDT input file> <RDF output file> 


** The tool hdtSearch allows to search triple patterns against an HDT file. For example, to list all patterns, one can use the "? ? ?" query. To search all information about <myns:subject1> one can use "<myns:subject1> ? ?"
$ hdtSearch <HDT File> 

** The tool hdtsparql searchs using a SPARQL Query. You must enclose the query in quotes so it is interpreted as a single parameter.
$ hdtsparql <HDT File> "SPARQL QUERY"

** The tool hdtInfo extracts the header from an HDT file.
$ hdtInfo <HDT File>


Tool Usage example
=================================

After installation, run:

# To convert an RDF file to HDT
$ rdf2hdt -base "http://example.org/test" data/test.nt data/test.hdt

# To extract the Header of the generated HDT
$ hdtInfo data/test.hdt
<http://example.org/test> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> "<http://purl.org/HDT/hdt#Dataset>" .
<http://example.org/test> <http://rdfs.org/ns/void#triples> "10" .
.....

# To convert back the HDT to RDF.
$ hdt2rdf data/test.hdt data/test.hdtexport.nt

# To search against an HDT file.
$ hdtSearch data/test.hdt

>> ? ? ?
http://example.org/uri3 http://example.org/predicate3 http://example.org/uri4
http://example.org/uri3 http://example.org/predicate3 http://example.org/uri5
http://example.org/uri4 http://example.org/predicate4 http://example.org/uri5
http://example.org/uri1 http://example.org/predicate1 "literal1"
http://example.org/uri1 http://example.org/predicate1 "literalA"
http://example.org/uri1 http://example.org/predicate1 "literalB"
http://example.org/uri1 http://example.org/predicate1 "literalC"
http://example.org/uri1 http://example.org/predicate2 http://example.org/uri3
http://example.org/uri1 http://example.org/predicate2 http://example.org/uriA3
http://example.org/uri2 http://example.org/predicate1 "literal1"
9 results shown.

>> http://example.org/uri3 ? ?
http://example.org/uri3 http://example.org/predicate3 http://example.org/uri4
http://example.org/uri3 http://example.org/predicate3 http://example.org/uri5
2 results shown.

>> exit

# To search a SPARQL Query
$ hdtsparql data/test.hdt "SELECT ?s ?p ?o WHERE { ?s ?p ?o }"


USING THE LIBRARY
=========================

Here we show how to use the library programmatically from java.

// Generate an HDT File from RDF (examples/ExampleGenerate.java)
public static void main(String[] args) throws Exception {
	// Configuration variables
	String baseURI = "http://example.com/mydataset";
	String rdfInput = "/path/to/dataset.nt";
	String inputType = "ntriples";
	String hdtOutput = "/path/to/dataset.hdt";
	
	// Create HDT from RDF file
	HDT hdt = HDTManager.generateHDT(rdfInput, baseURI, RDFNotation.parse(inputType), new HDTSpecification(), null);
	
	// Save generated HDT to a file
	hdt.saveToHDT(hdtOutput, null);
}

// Load an HDT and perform a search. (examples/ExampleSearch.java)
public static void main(String[] args) throws Exception {
	// Load HDT file. NOTE: Use loadIndexedHDT() if you are doing ?P?, ?PO or ??O queries
	HDT hdt = HDTManager.loadHDT("data/example.hdt", null);

	// Search pattern: Empty string means "any"
	IteratorTripleString it = hdt.search("", "", "");
	while(it.hasNext()) {
		TripleString ts = it.next();
		System.out.println(ts);
	}
}


USING HDT FILES FROM APACHE JENA
================================

Note: You need to add to the classpath: hdt-lib.jar, hdt-jena.jar and Jena jars (Including Jena-ARQ if you want SPARQL).

// Load HDT file using the hdt-java library
HDT hdt = HDTManager.mapIndexedHDT("path/to/file.hdt", null);

// Create Jena Model on top of HDT.
HDTGraph graph = new HDTGraph(hdt);
Model model = new ModelCom(graph);

// Use Jena Model as Read-Only data storage, e.g. Using Jena ARQ for SPARQL.


CREATE SPARQL ENDPOINT OF HDT FILES USING JENA FUSEKI
===================================================================

# You need to create a fuseki config file specifying which HDT files you want to publish. You can indicate one for the default graph and zero or more named graphs with additional HDT files. You also need to ask fuseki to load the HDT Assembler class:

[]   ja:loadClass "org.rdfhdt.hdtjena.HDTGraphAssembler" .

# An example config file is available at fuseki_example.ttl

# Download Fuseki, and edit the launch script to append hdt-java.jar and hdt-jena.jar to the classpath. Please note that if you use the "java -jar" option it ignores the classpath, so you'll end up with a line like this in your fuseki-server launch script:
exec java  $JVM_ARGS -classpath "hdt-lib.jar:hdt-jena.jar:$JAR" org.apache.jena.fuseki.FusekiCmd "$@"

or like this under windows:
java -Xmx1200M -classpath "hdt-lib.jar;hdt-jena.jar;fuseki-server.jar" org.apache.jena.fuseki.FusekiCmd %*

# Launch fuseki using:
$ ./fuseki-server --config=fuseki_example.ttl

# Try on a web browser:
http://localhost:3030
